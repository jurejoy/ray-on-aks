# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START gke_ai_ml_gke_ray_rayserve_llama2_7b]
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: rayllm
spec:
  serviceUnhealthySecondThreshold: 60 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 60 # Config for the health check threshold for deployments. Default value is 60.
  serveConfig:
  rayClusterConfig:
    # Ray head pod template
    headGroupSpec:
      # The `rayStartParams` are used to configure the `ray start` command.
      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
      rayStartParams:
        resources: '"{\"accelerator_type_cpu\": 2}"'
        dashboard-host: '0.0.0.0'
        block: 'true'
      #pod template
      template:
        spec:
          containers:
          - name: ray-head
            image: anyscale/ray-llm:2.44.0-py311-cu124
            resources:
              limits:
                cpu: "2"
                memory: "8Gi"
              requests:
                cpu: "2"
                memory: "8Gi"
            volumeMounts:
            - mountPath: /home/ray/models
              name: model
            - mountPath: /home/ray/serve_llama2.py
              name: serve-llama2-script
              subPath: serve_llama2.py
            ports:
            - containerPort: 6379
              name: gcs-server
            - containerPort: 8265 # Ray dashboard
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
          volumes:
          - name: model
            configMap:
              name: meta-llama2-7b-chat-hf
              items:
              - key: meta-llama--Llama-2-7b-chat-hf.yaml
                path: meta-llama--Llama-2-7b-chat-hf.yaml
          - name: serve-llama2-script
            configMap:
              name: serve-llama2-script
          # Please ensure the following taint has been applied to the GPU node in the cluster.
          tolerations:
            - key: sku
              operator: Exists
              effect: NoSchedule
            - key: CriticalAddonsOnly
              operator: Exists
              effect: NoSchedule
            # Tolerate the Azure spot node taint
            - key: kubernetes.azure.com/scalesetpriority
              operator: Equal
              value: "spot"
              effect: NoSchedule              
    workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 1
      minReplicas: 0
      maxReplicas: 1
      # logical group name, for this called small-group, also can be functional
      groupName: gpu-group
      rayStartParams:
        block: 'true'
        resources: '"{\"accelerator_type_cpu\": 20, \"accelerator_type_a100\": 1}"'
      # pod template
      template:
        spec:
          containers:
          - name: llm
            image: anyscale/ray-llm:2.44.0-py311-cu124
            env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-secret
                  key: hf_api_token
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            volumeMounts:
            - mountPath: /home/ray/models
              name: model
            - mountPath: /home/ray/serve_llama2.py
              name: serve-llama2-script
              subPath: serve_llama2.py
            - mountPath: /root/.cache/huggingface
              name: hf-cache
            resources:
              limits:
                cpu: "20"
                memory: "100Gi"
                nvidia.com/gpu: "1"
              requests:
                cpu: "20"
                memory: "100Gi"
                nvidia.com/gpu: "1"
          # Please ensure the following taint has been applied to the GPU node in the cluster.
          tolerations:
            - key: sku
              operator: Exists
              effect: NoSchedule
            - key: CriticalAddonsOnly
              operator: Exists
              effect: NoSchedule
            # Tolerate the Azure spot node taint
            - key: kubernetes.azure.com/scalesetpriority
              operator: Equal
              value: "spot"
              effect: NoSchedule
          volumes:
          - name: model
            configMap:
              name: meta-llama2-7b-chat-hf
              items:
              - key: meta-llama--Llama-2-7b-chat-hf.yaml
                path: meta-llama--Llama-2-7b-chat-hf.yaml
          - name: serve-llama2-script
            configMap:
              name: serve-llama2-script
          - name: hf-cache
            persistentVolumeClaim:
              claimName: hf-cache-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hf-cache-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: managed-premium
  resources:
    requests:
      storage: 3000Gi
